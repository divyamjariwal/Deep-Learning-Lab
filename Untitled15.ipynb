{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divyamjariwal/Deep-Learning-Lab/blob/main/Untitled15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETAwvyQ2Jcgp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72aae093"
      },
      "source": [
        "# Task\n",
        "Implement an Artificial Neural Network (ANN) from scratch using NumPy to classify the MNIST dataset. The implementation should include data loading and preprocessing (normalization, flattening, one-hot encoding), initialization of network parameters (weights and biases), forward and backward propagation with ReLU and Softmax activation functions, and a training loop using gradient descent. Finally, evaluate the model's performance on the test set and visualize the training loss and accuracy over epochs. Summarize the ANN implementation, architecture, training process, and final performance on the MNIST dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6f43e36"
      },
      "source": [
        "## Load and Preprocess MNIST Data\n",
        "\n",
        "### Subtask:\n",
        "Load the MNIST dataset, normalize pixel values, flatten images, and one-hot encode labels for both training and testing sets. This step will prepare the data for the neural network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80e18971"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I'll import the necessary libraries, then load the MNIST dataset, normalize pixel values, flatten the images, and one-hot encode the labels for both training and testing sets as per the instructions. This prepares the data for the neural network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27940f56",
        "outputId": "ef5885ea-3763-4c9b-ff9e-082289d76d26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "MNIST data loaded and preprocessed successfully.\n",
            "x_train shape: (60000, 784)\n",
            "y_train shape: (60000, 10)\n",
            "x_test shape: (10000, 784)\n",
            "y_test shape: (10000, 10)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# 1. and 2. Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# 3. Normalize pixel values\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# 4. Flatten images\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "# 5. and 6. One-hot encode labels\n",
        "num_classes = 10\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "print(\"MNIST data loaded and preprocessed successfully.\")\n",
        "print(f\"x_train shape: {x_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70cecc5c"
      },
      "source": [
        "## Initialize Neural Network Parameters\n",
        "\n",
        "### Subtask:\n",
        "Initialize the weights and biases for all layers of the neural network using NumPy. This will involve defining the architecture (e.g., number of layers, neurons per layer) and randomly initializing parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35e713a9"
      },
      "source": [
        "### Implement `initialize_parameters` function\n",
        "\n",
        "This function will set up the neural network's architecture by defining the number of layers and neurons, and then initializing the weights with small random values and biases with zeros for each layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0c5c1e2"
      },
      "source": [
        "**Reasoning**:\n",
        "Now I will implement the `initialize_parameters` function as described, define the network architecture, and then call this function to initialize the weights and biases for the neural network. This fulfills the subtask requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0edde216",
        "outputId": "c7b22681-0636-4a0f-bc7c-8afa9a75c160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters initialized successfully.\n",
            "Shape of W1: (128, 784)\n",
            "Shape of b1: (128, 1)\n",
            "Shape of W2: (10, 128)\n",
            "Shape of b2: (10, 1)\n"
          ]
        }
      ],
      "source": [
        "def initialize_parameters(layer_dims):\n",
        "    \"\"\"\n",
        "    Initializes parameters for the neural network.\n",
        "\n",
        "    Arguments:\n",
        "    layer_dims -- python list containing the dimensions of each layer in our network\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
        "    \"\"\"\n",
        "    np.random.seed(3) # for reproducibility\n",
        "    parameters = {}\n",
        "    L = len(layer_dims) # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "    return parameters\n",
        "\n",
        "# 1. Define the neural network architecture\n",
        "input_size = x_train.shape[1] # 784 features for MNIST images\n",
        "hidden_layer_size = 128 # Example hidden layer size\n",
        "output_size = num_classes # 10 classes for MNIST\n",
        "\n",
        "# Define the architecture as a list of layer dimensions\n",
        "# (input_layer_size, hidden_layer_1_size, ..., output_layer_size)\n",
        "layer_dims = [input_size, hidden_layer_size, output_size]\n",
        "\n",
        "# 6. Call this function to get the initial parameters\n",
        "parameters = initialize_parameters(layer_dims)\n",
        "\n",
        "print(\"Parameters initialized successfully.\")\n",
        "print(f\"Shape of W1: {parameters['W1'].shape}\")\n",
        "print(f\"Shape of b1: {parameters['b1'].shape}\")\n",
        "print(f\"Shape of W2: {parameters['W2'].shape}\")\n",
        "print(f\"Shape of b2: {parameters['b2'].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70646e0e"
      },
      "source": [
        "## Implement Activation Functions\n",
        "\n",
        "### Subtask:\n",
        "Define essential activation functions such as ReLU for hidden layers and Softmax for the output layer, along with their respective derivatives, which are crucial for backpropagation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ce4c81a"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define the `relu`, `relu_backward`, and `softmax` functions as specified in the instructions to implement the required activation functions and their derivatives.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63ad910e",
        "outputId": "9244b68d-5b64-4630-846c-c0976ee6bc44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation functions (ReLU and Softmax) and their derivatives defined successfully.\n"
          ]
        }
      ],
      "source": [
        "def relu(Z):\n",
        "    \"\"\"\n",
        "    Implement the ReLU activation function.\n",
        "\n",
        "    Arguments:\n",
        "    Z -- Output of the linear layer, of any shape\n",
        "\n",
        "    Returns:\n",
        "    A -- Post-activation parameter, of the same shape as Z\n",
        "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    A = np.maximum(0, Z)\n",
        "    return A, Z\n",
        "\n",
        "def relu_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single ReLU unit.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True)  # initializing dZ as a copy of dA\n",
        "    dZ[Z <= 0] = 0\n",
        "    return dZ\n",
        "\n",
        "def softmax(Z):\n",
        "    \"\"\"\n",
        "    Implement the Softmax activation function.\n",
        "\n",
        "    Arguments:\n",
        "    Z -- Output of the linear layer, of any shape\n",
        "\n",
        "    Returns:\n",
        "    A -- Post-activation parameter, of the same shape as Z\n",
        "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    # Subtract max for numerical stability\n",
        "    exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
        "    A = exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
        "    return A, Z\n",
        "\n",
        "print(\"Activation functions (ReLU and Softmax) and their derivatives defined successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89e46e88"
      },
      "source": [
        "## Implement Forward Propagation\n",
        "\n",
        "### Subtask:\n",
        "Write the function for the forward pass, which takes input data and calculates the outputs of each layer sequentially, leading to the final prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe134dfc"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define the `linear_forward`, `linear_activation_forward`, and `forward_propagation` functions as specified in the instructions to implement the forward pass for the neural network. This will involve computing the linear transformation and applying the appropriate activation function for each layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e109db6c",
        "outputId": "c41d0d13-0d02-4f9b-8cc9-69589ac165a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward propagation functions (linear_forward, linear_activation_forward, forward_propagation) defined successfully.\n"
          ]
        }
      ],
      "source": [
        "def linear_forward(A_prev, W, b):\n",
        "    \"\"\"\n",
        "    Implement the linear part of a layer's forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: (size of current layer, size of previous layer)\n",
        "    b -- bias vector: (size of current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- the input of the activation function, also called pre-activation parameter\n",
        "    cache -- a python dictionary containing \"A_prev\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    Z = np.dot(W, A_prev) + b\n",
        "\n",
        "    assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (A_prev, W, b)\n",
        "\n",
        "    return Z, cache\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
        "\n",
        "    Arguments:\n",
        "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: (size of current layer, size of previous layer)\n",
        "    b -- bias vector: (size of current layer, 1)\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"relu\" or \"softmax\"\n",
        "\n",
        "    Returns:\n",
        "    A -- the output of the activation function, also called the post-activation value\n",
        "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
        "             stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "\n",
        "    if activation == \"relu\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)\n",
        "    elif activation == \"softmax\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = softmax(Z)\n",
        "\n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache\n",
        "\n",
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation\n",
        "\n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (input size, number of examples)\n",
        "    parameters -- output of initialize_parameters()\n",
        "\n",
        "    Returns:\n",
        "    AL -- last post-activation value\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_relu_forward() (it's caches[l-1] for layer l)\n",
        "                the cache of linear_softmax_forward() (it's caches[L-1])\n",
        "    \"\"\"\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2  # number of layers in the neural network\n",
        "\n",
        "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
        "    for l in range(1, L):\n",
        "        A_prev = A\n",
        "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
        "        caches.append(cache)\n",
        "\n",
        "    # Implement LINEAR -> SOFTMAX. Add \"cache\" to the \"caches\" list.\n",
        "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"softmax\")\n",
        "    caches.append(cache)\n",
        "\n",
        "    assert(AL.shape == (parameters['W' + str(L)].shape[0], X.shape[1]))\n",
        "\n",
        "    return AL, caches\n",
        "\n",
        "print(\"Forward propagation functions (linear_forward, linear_activation_forward, forward_propagation) defined successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f374379"
      },
      "source": [
        "## Implement the Cost Function\n",
        "\n",
        "### Subtask:\n",
        "Implement a function to compute the categorical cross-entropy cost, which quantifies the difference between the predicted probabilities and the true labels. This cost function will be used to evaluate the model's performance during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36d3ceda"
      },
      "source": [
        "## Implement the Cost Function\n",
        "\n",
        "### Subtask:\n",
        "Implement a function to compute the categorical cross-entropy cost, which quantifies the difference between the predicted probabilities and the true labels. This cost function will be used to evaluate the model's performance during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a33d4f50"
      },
      "source": [
        "## Implement the Cost Function\n",
        "\n",
        "### Subtask:\n",
        "Implement a function to compute the categorical cross-entropy cost, which quantifies the difference between the predicted probabilities and the true labels. This cost function will be used to evaluate the model's performance during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74404527"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define the `compute_cost` function to calculate the categorical cross-entropy cost, as this is essential for evaluating the model's performance and guiding the optimization process during training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43715270",
        "outputId": "fefc6939-b855-4f2f-b74d-7d861650bb5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost function (compute_cost) defined successfully.\n"
          ]
        }
      ],
      "source": [
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function using categorical cross-entropy.\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (number of classes, number of examples)\n",
        "    Y -- true \"label\" vector (one-hot encoded), shape (number of classes, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from AL and Y\n",
        "    # To avoid log(0) which is undefined, we clip AL values to a small epsilon\n",
        "    AL = np.clip(AL, 1e-10, 1 - 1e-10)\n",
        "    cost = -np.sum(Y * np.log(AL)) / m\n",
        "\n",
        "    cost = np.squeeze(cost)      # To make sure cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(cost.shape == ())\n",
        "\n",
        "    return cost\n",
        "\n",
        "print(\"Cost function (compute_cost) defined successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb9e265c"
      },
      "source": [
        "## Implement Backward Propagation\n",
        "\n",
        "### Subtask:\n",
        "Implement the backward pass functions to compute the gradients of the cost with respect to the weights and biases for each layer. This will involve defining functions for `linear_backward`, `relu_backward`, `softmax_backward` (implicitly handled by cross-entropy derivative), and `backward_propagation`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b37c806"
      },
      "source": [
        "**Reasoning**:\n",
        "I will implement the `linear_backward` and `backward_propagation` functions as specified, which are crucial for calculating gradients during the backward pass of the neural network. The `relu_backward` function is already defined.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b14388c",
        "outputId": "c981d3f5-8cc4-4ad3-dd7d-005cfdc1f125"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backward propagation functions (linear_backward, linear_activation_backward, backward_propagation) defined successfully.\n"
          ]
        }
      ],
      "source": [
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = dZ.shape[1]\n",
        "\n",
        "    dW = 1./m * np.dot(dZ, A_prev.T)\n",
        "    db = 1./m * np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "\n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
        "\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient for current layer l\n",
        "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"relu\" or \"softmax\"\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    linear_cache, activation_cache = cache\n",
        "\n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "    elif activation == \"softmax\":\n",
        "        # dA for softmax is already dAL - Y, so we use it directly as dZ\n",
        "        # In our case, the derivative of softmax with respect to Z is already\n",
        "        # incorporated into the `dA` provided by the cost function's backward pass.\n",
        "        # So, dZ for softmax is directly dA.\n",
        "        dZ = dA\n",
        "\n",
        "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def backward_propagation(AL, Y, caches):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the entire network.\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector (one-hot encoded) (output of load_data())\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_relu_forward() (it's caches[l-1] for layer l)\n",
        "                the cache of linear_softmax_forward() (it's caches[L-1])\n",
        "\n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ...\n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # ensure Y is the same shape as AL\n",
        "\n",
        "    # Initializing the backpropagation\n",
        "    # dAL for softmax is (AL - Y), as the derivative of categorical cross-entropy with softmax is AL - Y\n",
        "    dAL = AL - Y\n",
        "\n",
        "    # Lth layer (SOFTMAX -> LINEAR) gradients\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"softmax\")\n",
        "\n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads\n",
        "\n",
        "print(\"Backward propagation functions (linear_backward, linear_activation_backward, backward_propagation) defined successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b3933e6"
      },
      "source": [
        "## Implement the Training Loop\n",
        "\n",
        "### Subtask:\n",
        "Implement the training loop using gradient descent. This involves repeatedly performing forward propagation, computing the cost, executing backward propagation to get gradients, and updating the parameters (weights and biases) for a specified number of epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b658a7f5"
      },
      "source": [
        "## Implement the Training Loop\n",
        "\n",
        "### Subtask:\n",
        "Implement the training loop using gradient descent. This involves repeatedly performing forward propagation, computing the cost, executing backward propagation to get gradients, and updating the parameters (weights and biases) for a specified number of epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31a42a96"
      },
      "source": [
        "## Implement the Training Loop\n",
        "\n",
        "### Subtask:\n",
        "Implement the training loop using gradient descent. This involves repeatedly performing forward propagation, computing the cost, executing backward propagation to get gradients, and updating the parameters (weights and biases) for a specified number of epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef7f72ac"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define the `update_parameters` function, which is a crucial component of the training loop for adjusting the network's weights and biases based on the calculated gradients.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "195d96dd",
        "outputId": "9983e1fc-4ff6-4d71-e0a9-b2efc7707861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter update function (update_parameters) and prediction function (predict) defined successfully.\n"
          ]
        }
      ],
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Updates parameters using gradient descent update rule.\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters\n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    learning_rate -- the learning rate, a scalar\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters\n",
        "    \"\"\"\n",
        "\n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter\n",
        "    for l in range(1, L + 1):\n",
        "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n",
        "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
        "    return parameters\n",
        "\n",
        "# Define a helper function to calculate accuracy\n",
        "def predict(X, parameters):\n",
        "    \"\"\"\n",
        "    This function is used to predict the results of a  L-layer neural network.\n",
        "\n",
        "    Arguments:\n",
        "    X -- data set of examples you would like to label\n",
        "    parameters -- parameters of the trained model\n",
        "\n",
        "    Returns:\n",
        "    predictions -- predictions for the given dataset X\n",
        "    \"\"\"\n",
        "\n",
        "    m = X.shape[1]\n",
        "    n = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Forward propagation\n",
        "    probas, caches = forward_propagation(X, parameters)\n",
        "\n",
        "    # Convert probabilities to actual predictions\n",
        "    predictions = np.argmax(probas, axis=0)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "print(\"Parameter update function (update_parameters) and prediction function (predict) defined successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc234a5b"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define the main `L_layer_model` function that orchestrates the entire training process. This function will integrate forward propagation, cost computation, backward propagation, and parameter updates over a specified number of epochs, and also track the loss and accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d555805",
        "outputId": "fd8459c8-542d-48f1-8729-1833dce6dd9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L_layer_model (training loop) function defined successfully.\n"
          ]
        }
      ],
      "source": [
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost = False, print_accuracy = False):\n",
        "    \"\"\"\n",
        "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX.\n",
        "\n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (input size, number of examples)\n",
        "    Y -- true \"label\" vector (one-hot encoded), of shape (number of classes, number of examples)\n",
        "    layers_dims -- list containing the input size and each layer's size, of length (number of layers + 1).\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    num_iterations -- number of iterations to run the optimization loop\n",
        "    print_cost -- if True, it prints the cost every 100 steps\n",
        "    print_accuracy -- if True, it prints the accuracy every 100 steps\n",
        "\n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    costs -- list of costs over iterations\n",
        "    train_accuracies -- list of training accuracies over iterations\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(1) # for reproducibility\n",
        "    costs = []                         # keep track of cost\n",
        "    train_accuracies = []              # keep track of training accuracy\n",
        "\n",
        "    # Parameters initialization.\n",
        "    parameters = initialize_parameters(layers_dims)\n",
        "\n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SOFTMAX.\n",
        "        AL, caches = forward_propagation(X, parameters)\n",
        "\n",
        "        # Compute cost.\n",
        "        cost = compute_cost(AL, Y)\n",
        "\n",
        "        # Backward propagation.\n",
        "        grads = backward_propagation(AL, Y, caches)\n",
        "\n",
        "        # Update parameters.\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "        # Print the cost and accuracy every 100 training examples\n",
        "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
        "            print(f\"Cost after iteration {i}: {cost}\")\n",
        "            costs.append(cost)\n",
        "        if print_accuracy and i % 100 == 0 or i == num_iterations - 1:\n",
        "            predictions_train = predict(X, parameters)\n",
        "            true_labels_train = np.argmax(Y, axis=0)\n",
        "            train_accuracy = np.mean(predictions_train == true_labels_train) * 100\n",
        "            print(f\"Train accuracy after iteration {i}: {train_accuracy:.2f}%\")\n",
        "            train_accuracies.append(train_accuracy)\n",
        "\n",
        "    return parameters, costs, train_accuracies\n",
        "\n",
        "print(\"L_layer_model (training loop) function defined successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41b50125"
      },
      "source": [
        "## Evaluate Model Performance\n",
        "\n",
        "### Subtask:\n",
        "After training, evaluate the performance of the neural network on the test dataset. Calculate metrics such as accuracy to assess how well the model generalizes to unseen data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3bbca86"
      },
      "source": [
        "**Reasoning**:\n",
        "I will call the `L_layer_model` function to train the neural network using the preprocessed training data and the defined architecture. This step will execute the forward and backward propagation cycles, update parameters, and track cost and accuracy over epochs, which is essential before evaluating on the test set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fcc5686",
        "outputId": "f66583a4-b2ed-45c8-c31f-8c22a42f363c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x_train before transpose: (60000, 784)\n",
            "Shape of y_train before transpose: (60000, 10)\n",
            "Shape of X_train_T after transpose: (784, 60000)\n",
            "Shape of Y_train_T after transpose: (10, 60000)\n",
            "Cost after iteration 0: 2.302442957095392\n",
            "Train accuracy after iteration 0: 8.95%\n",
            "Cost after iteration 100: 2.289347268205952\n",
            "Train accuracy after iteration 100: 42.59%\n",
            "Cost after iteration 200: 2.26486110878442\n",
            "Train accuracy after iteration 200: 47.60%\n",
            "Cost after iteration 300: 2.213510124411725\n",
            "Train accuracy after iteration 300: 49.38%\n",
            "Cost after iteration 400: 2.1127549827421968\n",
            "Train accuracy after iteration 400: 55.84%\n",
            "Cost after iteration 500: 1.9407679408976544\n",
            "Train accuracy after iteration 500: 64.93%\n",
            "Cost after iteration 600: 1.6982791686007352\n",
            "Train accuracy after iteration 600: 69.38%\n",
            "Cost after iteration 700: 1.4362668526053433\n",
            "Train accuracy after iteration 700: 71.08%\n",
            "Cost after iteration 800: 1.215117278670913\n",
            "Train accuracy after iteration 800: 73.61%\n",
            "Cost after iteration 900: 1.0486642882406196\n",
            "Train accuracy after iteration 900: 76.55%\n",
            "Cost after iteration 1000: 0.9252776638876761\n",
            "Train accuracy after iteration 1000: 78.78%\n",
            "Cost after iteration 1100: 0.8322573597307599\n",
            "Train accuracy after iteration 1100: 80.38%\n",
            "Cost after iteration 1200: 0.7605641015393535\n",
            "Train accuracy after iteration 1200: 81.60%\n",
            "Cost after iteration 1300: 0.7040828291924924\n",
            "Train accuracy after iteration 1300: 82.57%\n",
            "Cost after iteration 1400: 0.6586479501411433\n",
            "Train accuracy after iteration 1400: 83.44%\n",
            "Cost after iteration 1499: 0.6217268626893185\n",
            "Train accuracy after iteration 1499: 84.27%\n",
            "\n",
            "Training complete. Model parameters, costs, and training accuracies obtained.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Shape of x_train before transpose: {x_train.shape}\")\n",
        "print(f\"Shape of y_train before transpose: {y_train.shape}\")\n",
        "\n",
        "# Transpose x_train and y_train for the model\n",
        "X_train_T = x_train.T\n",
        "Y_train_T = y_train.T\n",
        "\n",
        "print(f\"Shape of X_train_T after transpose: {X_train_T.shape}\")\n",
        "print(f\"Shape of Y_train_T after transpose: {Y_train_T.shape}\")\n",
        "\n",
        "# 2. Call the L_layer_model function\n",
        "# The layer_dims were already defined in a previous step as [input_size, hidden_layer_size, output_size]\n",
        "parameters, costs, train_accuracies = L_layer_model(X_train_T, Y_train_T, layer_dims, learning_rate = 0.01, num_iterations = 1500, print_cost = True, print_accuracy = True)\n",
        "\n",
        "print(\"\\nTraining complete. Model parameters, costs, and training accuracies obtained.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+X1vgSlnh6SEA/OoEEUJd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}